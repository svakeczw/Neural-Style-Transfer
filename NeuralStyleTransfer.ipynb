{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/svakeczw/Neural-Style-Transfer/blob/main/NeuralStyleTransfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AN7NwQu2RYe8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import IPython.display as display\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O style.jpg https://github.com/svakeczw/Neural-Style-Transfer/raw/main/data/starry-night.jpg\n",
    "!wget -O content.jpg https://github.com/svakeczw/Neural-Style-Transfer/raw/main/data/syd-opera-house.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rY3Js45SSTO0"
   },
   "outputs": [],
   "source": [
    "content_path = 'content.jpg'\n",
    "style_path = 'style.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgufFmk7SaHF"
   },
   "outputs": [],
   "source": [
    "def view_img(img_path, img_title):\n",
    "  img = cv2.imread(img_path)\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  img = img / 255\n",
    "  plt.figure(figsize=(8,8))\n",
    "  plt.title(img_title)\n",
    "  plt.imshow(img)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    },
    "id": "xnSpwV00ScH7",
    "outputId": "5f69565f-3d5a-4235-e933-f4e3fc7e1416"
   },
   "outputs": [],
   "source": [
    "# view content image\n",
    "view_img(content_path, img_title='content')\n",
    "# view style image\n",
    "view_img(style_path, img_title='style')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6o56NhWSpd_"
   },
   "outputs": [],
   "source": [
    "# view the vgg layers\n",
    "# vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "# for layer in vgg.layers:\n",
    "  # print(layer.name)\n",
    "content_layers = ['block4_conv2']  # define content layers\n",
    "style_layers = [\n",
    "                'block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1',\n",
    "                'block4_conv1',\n",
    "                'block5_conv1'\n",
    "]  # define style layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18klu762Tmre"
   },
   "outputs": [],
   "source": [
    "# build customized vgg model\n",
    "def vgg_layers(layer_names):\n",
    "  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "  vgg.trainable = False\n",
    "  outputs = [vgg.get_layer(name).output for name in layer_names]\n",
    "  model = tf.keras.Model(inputs=vgg.input, outputs=outputs)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "186FrU8rTx5x"
   },
   "outputs": [],
   "source": [
    "# calculate gram matrix\n",
    "def gram_matrix(input_tensor):\n",
    "  ans = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "  input_shape = tf.shape(input_tensor)[1:]\n",
    "  num_elements = tf.cast(input_shape[0] * input_shape[1], tf.float32)\n",
    "  ans = ans / num_elements\n",
    "  return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boy4vwMTT1nB"
   },
   "outputs": [],
   "source": [
    "# Style transfer model\n",
    "class StyleTransferModel(tf.keras.Model):\n",
    "  def __init__(self, style_layers, content_layers):\n",
    "    super(StyleTransferModel, self).__init__()\n",
    "    self.style_layers = style_layers\n",
    "    self.content_layers = content_layers\n",
    "    self.n = len(style_layers)  # num of style layers\n",
    "    self.vgg = vgg_layers(style_layers + content_layers)\n",
    "    self.vgg.trainable = False\n",
    "\n",
    "  def call(self, inputs):\n",
    "    inputs = inputs * 255\n",
    "    inputs = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
    "\n",
    "\n",
    "    outputs = self.vgg(inputs)\n",
    "    style_outputs, content_outputs = outputs[:self.n], outputs[self.n:]\n",
    "\n",
    "    style_outputs = [gram_matrix(output_style) for output_style in style_outputs]\n",
    "    \n",
    "    content_dic = {k:v for k,v in zip(self.content_layers, content_outputs)}\n",
    "\n",
    "    style_dic = {k:v for k,v in zip(self.style_layers, style_outputs)}\n",
    "\n",
    "    return {'content': content_dic,\n",
    "            'style': style_dic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "teto4NUNUI9z"
   },
   "outputs": [],
   "source": [
    "# preprocess image\n",
    "def load_img(img_path, max_size):\n",
    "  img = tf.io.read_file(img_path)\n",
    "  img = tf.io.decode_image(img)\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)  # img / 255.0\n",
    "  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "  long_dim = max(shape)\n",
    "  if long_dim > max_size:\n",
    "    scale = max_size / long_dim\n",
    "    new_shape = tf.cast(shape * scale, tf.int32)\n",
    "    img = tf.image.resize(img, new_shape)\n",
    "  img = tf.expand_dims(img, axis=0)  # add batch dim\n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAkk5hhWU4yZ"
   },
   "outputs": [],
   "source": [
    "# clip the pixel value between 0 and 1\n",
    "def clip_img(image):\n",
    "  return tf.clip_by_value(image,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-j7WC5eUggw",
    "outputId": "c0c85fbb-a50e-41e4-9aa3-2bd8fb0d3e13"
   },
   "outputs": [],
   "source": [
    "stf_model = StyleTransferModel(style_layers, content_layers)\n",
    "\n",
    "style_image = load_img(style_path, 512)\n",
    "content_image = load_img(content_path, 512)\n",
    "\n",
    "\n",
    "style_target = stf_model(style_image)['style']  # get target style\n",
    "content_target = stf_model(content_image)['content']  # get target content\n",
    "\n",
    "generated_image = tf.Variable(content_image)  # initialize generated image as content image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wf0KgF2cVOuT"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.02)\n",
    "\n",
    "# weights for content cost and style cost\n",
    "content_cost_alpha = 1\n",
    "style_cost_beta = 100\n",
    "\n",
    "# weights for each layer\n",
    "style_layer_weights = [\n",
    "                       tf.constant(0.2, tf.float32),  # block1_conv1\n",
    "                       tf.constant(0.2, tf.float32),  # block2_conv1\n",
    "                       tf.constant(0.2, tf.float32),  # block3_conv1\n",
    "                       tf.constant(0.2, tf.float32),  # block4_conv1\n",
    "                       tf.constant(0.2, tf.float32),  # block5_conv1\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bSt25eRVvJY"
   },
   "outputs": [],
   "source": [
    "# content style loss\n",
    "def content_style_loss(outputs):\n",
    "  style_outputs = outputs['style']\n",
    "  content_outputs = outputs['content']\n",
    "\n",
    "\n",
    "  # mean square error loss\n",
    "  style_loss = [tf.reduce_mean((style_outputs[name] - style_target[name])**2) for name in style_outputs.keys()]\n",
    "  style_loss = [style_loss[i] * style_layer_weights[i] for i in range(len(style_outputs))] \n",
    "\n",
    "\n",
    "  style_loss = tf.reduce_sum(style_loss)\n",
    "  style_loss *= style_cost_beta\n",
    "\n",
    "  content_loss = [tf.reduce_mean((content_outputs[name] - content_target[name])**2) for name in content_outputs.keys()]\n",
    "  content_loss = tf.reduce_sum(content_loss)\n",
    "  content_loss *= content_cost_alpha\n",
    "\n",
    "  loss = content_loss + style_loss\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOuBSSVLWTPe"
   },
   "outputs": [],
   "source": [
    "# define one training step\n",
    "@tf.function()\n",
    "def train_step(image):\n",
    "  with tf.GradientTape() as tape:\n",
    "    outputs = stf_model(image)\n",
    "    loss = content_style_loss(outputs)\n",
    "  gradients = tape.gradient(loss, image)\n",
    "  optimizer.apply_gradients([(gradients, image)])\n",
    "  image.assign(clip_img(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAtrneplX-jO"
   },
   "outputs": [],
   "source": [
    "# display the image\n",
    "def show_tensor_img(img_tensor):\n",
    "  img = img_tensor * 255\n",
    "  img = np.asarray(img, dtype='uint8')\n",
    "  img = np.squeeze(img)\n",
    "  return PIL.Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Cnd7ZKR0YCM7",
    "outputId": "609f5a9a-2290-4466-b6a3-ed3ce5a36f79"
   },
   "outputs": [],
   "source": [
    "# training\n",
    "epoch = 10\n",
    "step_each_epoch = 100\n",
    "\n",
    "for i in range(epoch):\n",
    "  for _ in range(step_each_epoch):\n",
    "    train_step(generated_image)\n",
    "    if _ == 99:\n",
    "      # display.clear_output(wait=True)\n",
    "      print(f'Generated image after trained for {i+1} epoch')\n",
    "      display.display(show_tensor_img(generated_image))\n",
    "  print(f'epoch: {i+1} done !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img_style = cv2.imread(style_path)\n",
    "img_style = cv2.cvtColor(img_style, cv2.COLOR_BGR2RGB)\n",
    "img_style = img_style / 255\n",
    "\n",
    "img_content = cv2.imread(content_path)\n",
    "img_content = cv2.cvtColor(img_content, cv2.COLOR_BGR2RGB)\n",
    "img_content = img_content / 255\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(np.squeeze(generated_image))\n",
    "# plt.axis('off')\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(img_content)\n",
    "# plt.axis('off')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(img_style)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNhkQIy8IS3eHuh/gIEOYyp",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NeuralStyleTransfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}